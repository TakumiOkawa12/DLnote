{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ae516d-6168-4e3c-b3ae-fb7726d2da5c",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b4adbf-7c93-468d-9c0f-e521e80bc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71741695-7153-4a15-97bc-c0bbf0e814d2",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe28351-aa98-40bf-a069-a68315e8cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "  def __init__(self, in_ch, num_classes, transposed=True):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dconv_down1 = self._double_conv(in_ch, 64)\n",
    "    self.dconv_down2 = self._double_conv(64, 128)\n",
    "    self.dconv_down3 = self._double_conv(128, 256)\n",
    "    self.dconv_down4 = self._double_conv(256, 512)\n",
    "    self.dconv_down5 = self._double_conv(512, 1024)\n",
    "\n",
    "    self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    self.upconv4 = self._up_conv(1024, 512, transposed=transposed)\n",
    "    self.upconv3 = self._up_conv(512, 256, transposed=transposed)\n",
    "    self.upconv2 = self._up_conv(256, 128, transposed=transposed)\n",
    "    self.upconv1 = self._up_conv(128, 64, transposed=transposed)\n",
    "\n",
    "    self.dconv_up4 = self._double_conv(1024, 512)\n",
    "    self.dconv_up3 = self._double_conv(512, 256)\n",
    "    self.dconv_up2 = self._double_conv(256, 128)\n",
    "    self.dconv_up1 = self._double_conv(128, 64)\n",
    "\n",
    "    self.conv_last = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "\n",
    "  def _double_conv(self, in_ch, out_ch):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def _up_conv(self, in_ch, out_ch, transposed=True):\n",
    "    if transposed:\n",
    "      return nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "    else:\n",
    "      return nn.Sequential(\n",
    "          nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "          nn.Conv2d(in_ch, out_ch, 1))\n",
    "\n",
    "\n",
    "  def forward(self, X):\n",
    "    conv1 = self.dconv_down1(X)\n",
    "    X = self.maxpool(conv1)\n",
    "\n",
    "    conv2 = self.dconv_down2(X)\n",
    "    X = self.maxpool(conv2)\n",
    "\n",
    "    conv3 = self.dconv_down3(X)\n",
    "    X = self.maxpool(conv3)\n",
    "\n",
    "    conv4 = self.dconv_down4(X)\n",
    "    X = self.maxpool(conv4)\n",
    "\n",
    "    X = self.dconv_down5(X)\n",
    "\n",
    "    X = self.upconv4(X)\n",
    "    X = self.dconv_up4(torch.cat([X, conv4], dim=1))\n",
    "    X = self.upconv3(X)\n",
    "    X = self.dconv_up3(torch.cat([X, conv3], dim=1))\n",
    "    X = self.upconv2(X)\n",
    "    X = self.dconv_up2(torch.cat([X, conv2], dim=1))\n",
    "    X = self.upconv1(X)\n",
    "    X = self.dconv_up1(torch.cat([X, conv1], dim=1))\n",
    "\n",
    "    out = self.conv_last(X)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cfa4e-8e14-4d3e-a1a8-4f761e04c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力サイズは2^nを想定\n",
    "X = torch.randn(1, 3, 256, 256)\n",
    "model = UNet(3, 10, transposed=False)\n",
    "output = model(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06322802-9d13-4cbc-b104-7597ede8f1a9",
   "metadata": {},
   "source": [
    "### PASCAL VOCデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822965a1-c373-4d0a-958c-1cb22708a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_tensor(mask):\n",
    "  return (transforms.ToTensor()(mask) * 255).long()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    mask_to_tensor,\n",
    "])\n",
    "trainset = VOCSegmentation(root='./voc_data', year='2012', image_set='train', download=True, transform=transform, target_transform=target_transform)\n",
    "valset = VOCSegmentation(root='./voc_data', year='2012', image_set='val', download=True, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc45053-2100-4b1a-b64d-9a34aea31c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2511e-deb9-4c54-85c9-08726473b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = trainset[0]\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "image_denormalized = image.numpy() * std[:, None, None] + mean[:, None, None]\n",
    "image_clipped = np.clip(image_denormalized, 0, 1)\n",
    "image_rescaled = (image_clipped * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099aeaa-eab5-4e36-919c-3c8e989b53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_rescaled.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e0b74-42aa-41fb-a788-2307872d5f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~255の256段階なので，普通に画像を表示するだけだとあまりマスクの色に違いがみれない\n",
    "plt.imshow(mask.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e92fe-939a-42e4-9804-25e4dd5f039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_mask = color.label2rgb(mask[0].numpy(), image_rescaled.transpose(1, 2, 0))\n",
    "plt.imshow(colored_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f9b7c-d102-48dd-b928-ea036aacbd19",
   "metadata": {},
   "source": [
    "### 2クラス分類のデータセットを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6653a-4894-42cc-8d2f-c5efde23fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21クラス分類だと学習に時間がかかるため，ここではシンプルな2クラス分類(person vs 背景)にする\n",
    "class CustomVOCSegmentation(VOCSegmentation):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.person_class_id = 15\n",
    "        self.data, self.targets = self.filter_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        mask = self.targets[index]\n",
    "        return img, mask\n",
    "\n",
    "    def filter_dataset(self):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        for i in range(super().__len__()):\n",
    "            img, mask = super().__getitem__(i)\n",
    "            mask = (mask == self.person_class_id).long()\n",
    "            if torch.sum(mask) > 0:\n",
    "                new_data.append(img)\n",
    "                new_targets.append(mask)\n",
    "        return new_data, new_targets\n",
    "\n",
    "# Data Augmetnationを含む前処理\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(256, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(256, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10, fill=(0,)), # 余白は背景とする (255をセットし，損失関数でignore_index=255にするのでもOK)\n",
    "    mask_to_tensor,\n",
    "    transforms.Lambda(lambda x: x.squeeze(0)) # DataLoaderから取得する時点で[b, 1, h, w]ではなく[b, h, w]にする (train loopで処理してもよい)\n",
    "])\n",
    "\n",
    "# validationではdata augmentaitonをしないので，別途用意\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    mask_to_tensor,\n",
    "    transforms.Lambda(lambda x: x.squeeze(0))\n",
    "])\n",
    "trainset_person = CustomVOCSegmentation(root='./voc_data', year='2012', image_set='train', download=True, transform=train_transform, target_transform=train_target_transform)\n",
    "valset_person = CustomVOCSegmentation(root='./voc_data', year='2012', image_set='val', download=True, transform=val_transform, target_transform=val_target_transform)\n",
    "trainloader = DataLoader(trainset_person, batch_size=4, shuffle=True, num_workers=4) # 有料版では複数のスレッドを使用可能\n",
    "valloader = DataLoader(valset_person, batch_size=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2af87-a1e0-4c18-a6cd-730acc427e4a",
   "metadata": {},
   "source": [
    "### クラスの重み計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75be0e0-8bdc-496a-9f91-ef8dab1272d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# クラスの重みつけ計算\n",
    "num_classes = 2\n",
    "class_sample_counts = torch.zeros(num_classes, dtype=torch.int64)\n",
    "for _, masks in trainloader:\n",
    "    for mask in masks:\n",
    "        mask = mask[mask != 255] # すでにDatasetのWrapperクラスで対処ずみだが，21クラス分類にする場合は必要\n",
    "        class_sample_counts += torch.bincount(mask.flatten(), minlength=num_classes)\n",
    "class_sample_counts = class_sample_counts.float() + 1e-5\n",
    "# クラスの出現頻度の逆数を重みにする\n",
    "class_weights = 1. / class_sample_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc32389-c199-4a0d-9e76-4abe6113083a",
   "metadata": {},
   "source": [
    "### モデル，Optimizer, 損失関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5dab1-3414-4509-ab3b-efc8237a2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_ch=3, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=255, weight=class_weights) # すでにDatasetのWrapperクラスで対処ずみだが，21クラス分類にする場合はignoreする必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4369a0a-82ff-42af-a4c9-333a24c94345",
   "metadata": {},
   "source": [
    "### 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc61ed-614a-4309-a5d5-15ebeeacb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "save_interval = 10  # 10エポック毎にモデルを評価/保存\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(trainloader, total=len(trainloader), desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # lossが(b, h, w)しか受け付けない -> transformでtransforms.Lambda(lambda x: x.squeeze(0))を実施していれば不要\n",
    "        # masks = masks.squeeze(1)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, masks)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(trainloader):.4f}')\n",
    "\n",
    "    # 各10エポック毎にモデルの評価\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                # masks = masks.squeeze(1)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = loss_func(outputs, masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss / len(valloader):.4f}')\n",
    "\n",
    "        # モデルの保存\n",
    "        torch.save(model.state_dict(), f'unet_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3976ddc-0310-479e-94d5-ea5d3cd25435",
   "metadata": {},
   "source": [
    "### モデルの予測結果可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330a8e8-96bd-4730-9c70-4a730324e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの予測結果の描画\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, masks = next(iter(valloader))\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    outputs = model(images)\n",
    "    # ひとまず最も値が大きいクラスを出力とするが，実際にはアプリケーションに応じて閾値を決める\n",
    "    _, predicted_masks = torch.max(outputs, 1)\n",
    "\n",
    "    # TensorをGPUからCPUに移動する\n",
    "    images = images.cpu()\n",
    "    predicted_masks = predicted_masks.cpu()\n",
    "    masks = masks.cpu()\n",
    "\n",
    "    index = 2\n",
    "\n",
    "    image = images[index].permute(1, 2, 0)\n",
    "    predicted_mask = predicted_masks[index]\n",
    "    mask = masks[index]\n",
    "\n",
    "    # Tensor -> Numpy Array\n",
    "    image = image.numpy()\n",
    "    predicted_mask = predicted_mask.numpy()\n",
    "    mask = mask.numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "    # Plot image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].title.set_text('Original Image')\n",
    "\n",
    "     # Plot mask\n",
    "    ax[1].imshow(mask, cmap='gray')\n",
    "    ax[1].title.set_text('Ground Truth Mask')\n",
    "\n",
    "    # Plot prediction\n",
    "    ax[2].imshow(predicted_mask, cmap='gray')\n",
    "    ax[2].title.set_text('Predicted Mask')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dd38a-8a8e-4ea3-bd94-c3b4be495a6e",
   "metadata": {},
   "source": [
    "### モデルの出力を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0c6e1-4ac5-4173-bc5b-1b158f321c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map = outputs[index, 1, :, :]\n",
    "plt.imshow(pred_map.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
