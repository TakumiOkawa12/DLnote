{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdmglSA6ojQu"
   },
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsABtYlYouij"
   },
   "source": [
    "### 必要なパッケージをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1-OLCd2toySq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "169dc0b7-e651-47c0-a3f9-48cdb87f871e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting portalocker\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: portalocker\n",
      "Successfully installed portalocker-2.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "j2mEwOBrremM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "19f798ca-4595-42ea-9348-602524ce0a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CdosvG-o2P3"
   },
   "source": [
    "### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkeT6Nu5pCuj"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oOub5nvpXm8"
   },
   "outputs": [],
   "source": [
    "# データのロード\n",
    "data = Multi30k(split='train', language_pair=('de', 'en')) # [(ドイツ語キャプション1, 英語キャプション1),(ドイツ語キャプション2, 英語キャプション2),・・・]のようなreturnがある\n",
    "data = list(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpUWQn9apilg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# データの分割\n",
    "train_data, remaining = train_test_split(data, train_size=0.1, random_state=0)\n",
    "_, val_data = train_test_split(remaining, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRsZ-j57pmV0",
    "outputId": "6d8ae232-8fe2-4416-dd96-201334aa580b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900 2611\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40sPxUfcqCen",
    "outputId": "c844b5ea-e3bc-4ea8-841e-3c7a55a89670"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizerと辞書作成\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "token_transform = {} # トークンの辞書を用意\n",
    "vocab_transform = {} # トークンをインデックス番号に変換したものを格納する辞書を用意\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "specials=['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def yield_tokens(data_iter, language):\n",
    "    \"\"\"\n",
    "    data_iter : イテレータを指定\n",
    "    language : イテレータの言語を指定\n",
    "\n",
    "    return : トークンの辞書（言語ごとに作成される）\n",
    "    \"\"\"\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE:1} # Multi30kは0番目にドイツ語キャプション、1番目に英語キャプションが格納されている\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    token_transform[ln] = get_tokenizer('spacy', language=ln)\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(data, ln), specials=specials)\n",
    "    # 今回は全ての単語で辞書を作ってるので，unknownはない想定\n",
    "    vocab_transform[ln].set_default_index(vocab_transform[ln]['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3od-udRntv1x",
    "outputId": "f48b3679-b803-4179-a08d-34395c0f3c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'people', 'are', 'walking', 'on', 'a', 'striped', 'path', '.']\n",
      "[19, 22, 17, 42, 9, 4, 198, 297, 5]\n",
      "tensor([ 19,  22,  17,  42,   9,   4, 198, 297,   5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# DataLoader作成\n",
    "# DataLoaderを作るためには，\n",
    "# 1. token化\n",
    "print(token_transform[TGT_LANGUAGE](train_data[0][1]))\n",
    "# 2. 符号化\n",
    "print([vocab_transform[TGT_LANGUAGE][token] for token in token_transform[TGT_LANGUAGE](train_data[0][1])])\n",
    "# 3. tensor化\n",
    "print(torch.tensor([vocab_transform[TGT_LANGUAGE][token] for token in token_transform[TGT_LANGUAGE](train_data[0][1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv8NUeWk1a3r"
   },
   "outputs": [],
   "source": [
    "def data_propcess(data_iter):\n",
    "    data = []\n",
    "    for (src, tgt) in data_iter:\n",
    "        src_tensor = torch.tensor([vocab_transform[SRC_LANGUAGE][token] for token in token_transform[SRC_LANGUAGE](src)])\n",
    "        tgt_tensor = torch.tensor([vocab_transform[TGT_LANGUAGE][token] for token in token_transform[TGT_LANGUAGE](tgt)])\n",
    "        data.append((src_tensor, tgt_tensor))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYbztnYy4miU"
   },
   "outputs": [],
   "source": [
    "train_data_tensor = data_propcess(train_data)\n",
    "val_data_tensor = data_propcess(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXrsra2r4rf6",
    "outputId": "c37c9a60-d594-4b2f-c49f-52f230b7f5d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 21,  42,  77,  11,   6, 259, 237,   4]),\n",
       "  tensor([ 19,  22,  17,  42,   9,   4, 198, 297,   5])),\n",
       " (tensor([   5,   12,   70,   11,   13, 3256,   15,  428,   10,   26,  189,    4]),\n",
       "  tensor([   6,   12,    9,    4, 1347, 1226,  173,    4,  267,  328,    5])),\n",
       " (tensor([    5,    12,    10,  2012,    70, 13529]),\n",
       "  tensor([   6,   12,  581, 1066,   14,   27,  570,  359,    9,    5]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiLFCA5X4sbr"
   },
   "outputs": [],
   "source": [
    "# padding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch = []\n",
    "    tgt_batch = []\n",
    "    for src, tgt in batch:\n",
    "        # <bos>と<eos>を追加\n",
    "        src_batch.append(torch.cat([torch.tensor([vocab_transform[SRC_LANGUAGE][\"<bos>\"]]),\n",
    "                                    src,\n",
    "                                    torch.tensor([vocab_transform[SRC_LANGUAGE][\"<eos>\"]])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([vocab_transform[TGT_LANGUAGE][\"<bos>\"]]),\n",
    "                                    tgt,\n",
    "                                    torch.tensor([vocab_transform[TGT_LANGUAGE][\"<eos>\"]])], dim=0))\n",
    "\n",
    "    # padding\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=vocab_transform[SRC_LANGUAGE]['<pad>'])\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=vocab_transform[TGT_LANGUAGE]['<pad>'])\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_loader = DataLoader(train_data_tensor, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_data_tensor, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMxyICZc7OA4",
    "outputId": "eff2e1c2-2637-4328-ef33-b23b519aaaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 28]) torch.Size([16, 26])\n"
     ]
    }
   ],
   "source": [
    "src, tgt = next(iter(train_loader))\n",
    "print(src.shape, tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVXNik8s7QEt",
    "outputId": "96807944-7b55-46a5-f54a-9c43175d5585"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__T9UwnX-C9L"
   },
   "source": [
    "### モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN3SCo1qEZcT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, embedding_matrix=None, num_layers=1, rnn_type='LSTM', bidirectional=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # embedding layer追加 (vocab_size x embedding_dim)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) # embedding matrixで重みを初期化\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        input_size = embedding_dim\n",
    "\n",
    "        if rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError('Unsupported RNN type. Choose from [\"LSTM\", \"RNN\", \"GRU\", \"UGRNN\"]')\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size*self.num_directions, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2AehAutEirY"
   },
   "outputs": [],
   "source": [
    "# encoderとdecoder作成\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super().__init__(vocab_size, embedding_dim, hidden_size, hidden_size, num_layers=num_layers, rnn_type='LSTM', bidirectional=False)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        output_seq, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        self.output_size = vocab_size\n",
    "        super().__init__(vocab_size, embedding_dim, hidden_size, vocab_size, num_layers=num_layers, rnn_type='LSTM', bidirectional=False)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1) # [batch_size] -> [batch_size, 1]\n",
    "        embedded = self.embedding(input) # [batch_size, 1] -> [batch_size, 1, emb_dim]\n",
    "        output_seq, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output_seq: [batch_size, 1, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "        # cell: [num_layers, batch_size, hidden_size]\n",
    "        prediction = self.fc(output_seq.squeeze(1))\n",
    "        # prediction: [batch_size, vocab_size]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAmzADbOYBdW"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing=0.5):\n",
    "        batch_size = tgt.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        # encoderのforward\n",
    "        hidden, cell = self.encoder(src)\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "\n",
    "        input = tgt[:, 0] # <bos>\n",
    "        outputs =  torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)# [batch_size, seq_len, vocab_size]\n",
    "        # decoderのforward\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output: [batch_size, vocab_size]\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = random.random() < teacher_forcing\n",
    "            top1 = output.argmax(1) # greedy search\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4DNvQ-2cJrX"
   },
   "source": [
    "### 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InS0dJoGhQNE"
   },
   "outputs": [],
   "source": [
    "# 学習ループ\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, model_save_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (src, tgt) in enumerate(train_loader):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(src, tgt) # [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "            # outputs[:, 0]は<bos>なので無視（値を入れていない）\n",
    "            output_size = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].reshape(-1, output_size) # [batch_size, tgt_len, tgt_vocab_size] => [batch_size * tgt_len, tgt_vocab_size]\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            loss = criterion(outputs, tgt)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # 検証データを使用して検証エラーを計算\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_samples = 0\n",
    "        total_correct = 0\n",
    "        for src, tgt in val_loader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            outputs = model(src, tgt, 0) # torcher_forcingは0にする\n",
    "            # loss計算\n",
    "            output_size = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].reshape(-1, output_size)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # モデル保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'{model_save_path}/seq2seq_{epoch}')\n",
    "            print(f'Model saved with validation loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB1ZATu9zH46"
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = len(vocab_transform[SRC_LANGUAGE])\n",
    "tgt_vocab_size = len(vocab_transform[TGT_LANGUAGE])\n",
    "embedding_dim = 300\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "\n",
    "enc = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "model = Seq2Seq(enc, dec, device)\n",
    "\n",
    "# Optimizerと損失関数\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_transform[TGT_LANGUAGE]['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUcR7Q6j1Mop",
    "outputId": "60102af3-ed30-462c-bf2b-b6eafdf32294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive_path = '/content/drive'\n",
    "drive.mount(drive_path)\n",
    "model_save_path = f'{drive_path}/MyDrive/models/machine_translation_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJm39jKs2Mw_",
    "outputId": "91a9f63d-3bce-41ef-ec26-d4ce4d23d535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Training Loss: 4.5489\n",
      "Val Loss: 5.0947\n",
      "Model saved with validation loss: 835.5343\n",
      "Epoch 2/10, Average Training Loss: 4.3223\n",
      "Val Loss: 5.1255\n",
      "Epoch 3/10, Average Training Loss: 4.1514\n",
      "Val Loss: 4.9929\n",
      "Model saved with validation loss: 818.8406\n",
      "Epoch 4/10, Average Training Loss: 3.9867\n",
      "Val Loss: 4.9433\n",
      "Model saved with validation loss: 810.7090\n",
      "Epoch 5/10, Average Training Loss: 3.8281\n",
      "Val Loss: 4.9343\n",
      "Model saved with validation loss: 809.2225\n",
      "Epoch 6/10, Average Training Loss: 3.6722\n",
      "Val Loss: 4.9725\n",
      "Epoch 7/10, Average Training Loss: 3.5369\n",
      "Val Loss: 4.9882\n",
      "Epoch 8/10, Average Training Loss: 3.4056\n",
      "Val Loss: 4.9373\n",
      "Epoch 9/10, Average Training Loss: 3.2591\n",
      "Val Loss: 5.0002\n",
      "Epoch 10/10, Average Training Loss: 3.1248\n",
      "Val Loss: 5.1264\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer, criterion, num_epochs, model_save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoZy3hv62ThR"
   },
   "source": [
    "### モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMxP3faSBRQ1",
    "outputId": "0d1a4df8-037a-4b91-efcc-e1bdbdf83559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "epoch = 4\n",
    "model.load_state_dict(torch.load(f'{model_save_path}/seq2seq_{epoch}', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jXbsJGICPht"
   },
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqJVulK_CeLh"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, setence, device, max_len=50):\n",
    "\n",
    "    # 1. tokenize\n",
    "    tokenized = token_transform[SRC_LANGUAGE](sentence)\n",
    "    # 2. 符号化と<bos>, <eos>をつける\n",
    "    numericalized = [vocab_transform[SRC_LANGUAGE]['<bos>']] \\\n",
    "                    + [vocab_transform[SRC_LANGUAGE][token] for token in tokenized] \\\n",
    "                    + [vocab_transform[SRC_LANGUAGE]['<eos>']]\n",
    "\n",
    "    #Tensor化\n",
    "    numericalized = torch.LongTensor(numericalized).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encoderのforward\n",
    "    hidden, cell = model.encoder(numericalized)\n",
    "\n",
    "    # <bos> (最初のDecoderへの入力)\n",
    "    input = torch.LongTensor([vocab_transform[TGT_LANGUAGE]['<bos>']]).to(device)\n",
    "\n",
    "    # Decoderのforwardで文章生成\n",
    "    translated_sentence = []\n",
    "    for _ in range(max_len):\n",
    "        output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "        # output: [1, vocab_size]\n",
    "        # greedy search\n",
    "        best_guess = output.argmax(1).item()\n",
    "\n",
    "        # 生成されたのが<eos>なら翻訳終了\n",
    "        if best_guess == vocab_transform[TGT_LANGUAGE]['<eos>']:\n",
    "            break\n",
    "        best_word = vocab_transform[TGT_LANGUAGE].lookup_token(best_guess)\n",
    "        translated_sentence.append(best_word)\n",
    "        input = torch.LongTensor([best_guess]).to(device)\n",
    "    return ' '.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3TYOVqrDgen",
    "outputId": "24a298e9-2779-48d3-934c-004c62955780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young player in a a a a a to a the other in the . .\n"
     ]
    }
   ],
   "source": [
    "# A boat with several men on it is being pulled to the shore by a large team of horses.\n",
    "sentence = \"Ein Boot mit mehreren Männern darauf wird von einem großen Pferdegespann ans Ufer gezogen.\"\n",
    "translation = translate_sentence(model, sentence, device)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1pjCjARTC9mg",
    "outputId": "c03aef46-2b93-453d-d90d-8ae2fe8b7259"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[TGT_LANGUAGE].lookup_token(best_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcJun9k8DWDA",
    "outputId": "c79de8b8-7f64-42f2-e314-1ab4823e5152"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[SRC_LANGUAGE]['<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6faQL1l0DXDT"
   },
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlbQ2WlxRhYl"
   },
   "outputs": [],
   "source": [
    "def translate_sentence_beam_search(model, setence, device, max_len=50, k=10, alpha=0.7):\n",
    "\n",
    "    # 1. tokenize\n",
    "    tokenized = token_transform[SRC_LANGUAGE](sentence)\n",
    "    # 2. 符号化と<bos>, <eos>をつける\n",
    "    numericalized = [vocab_transform[SRC_LANGUAGE]['<bos>']] \\\n",
    "                    + [vocab_transform[SRC_LANGUAGE][token] for token in tokenized] \\\n",
    "                    + [vocab_transform[SRC_LANGUAGE]['<eos>']]\n",
    "\n",
    "    #Tensor化\n",
    "    numericalized = torch.LongTensor(numericalized).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encoderのforward\n",
    "    hidden, cell = model.encoder(numericalized)\n",
    "\n",
    "    # <bos> (最初のDecoderへの入力)\n",
    "    input = torch.LongTensor([vocab_transform[TGT_LANGUAGE]['<bos>']]).to(device)\n",
    "\n",
    "    # Decoderのforwardで文章生成\n",
    "    translated_sentence = []\n",
    "\n",
    "    beam = [(0, [vocab_transform[TGT_LANGUAGE]['<bos>']], hidden, cell)]\n",
    "    beam_log =[]\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for score, word_list, hidden, cell in beam:\n",
    "            if word_list[-1] == vocab_transform[TGT_LANGUAGE]['<eos>']:\n",
    "                all_candidates.append((score, word_list, hidden, cell))\n",
    "            else:\n",
    "                input = torch.tensor([word_list[-1]]).to(device)\n",
    "                output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "                # output: [1, vocab_size]\n",
    "                probs = torch.log_softmax(output, dim=-1)\n",
    "                probs = probs.view(-1)\n",
    "\n",
    "                for i, prob in enumerate(probs):\n",
    "                    next_score = (score * (len(word_list)**alpha) + prob.item()) / ((len(word_list)+1)**alpha)\n",
    "                    # (score, wordlist, hidden, cell)\n",
    "                    all_candidates.append((next_score, word_list + [i], hidden, cell))\n",
    "        all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        beam = all_candidates[:k]\n",
    "\n",
    "        # log 用\n",
    "        beam_log.append(cand[0]for cand in all_candidates[:k])\n",
    "    return beam, beam_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx9aaaJGVJin"
   },
   "outputs": [],
   "source": [
    "# A boat with several men on it is being pulled to the shore by a large team of horses.\n",
    "sentence = \"Ein Boot mit mehreren Männern darauf wird von einem großen Pferdegespann ans Ufer gezogen.\"\n",
    "beam, beam_log = translate_sentence_beam_search(model, sentence, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zC-_E0uRY8zD",
    "outputId": "328017ef-9e85-4ec6-93ed-8f3656715587"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBKWgHgGY5_8",
    "outputId": "96589a3f-e8e2-49b0-a349-02fb0816666c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'on', 'a', 'a', 'in', 'the', 'other', 'in', 'the', 'background', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'on', 'a', 'a', 'in', 'the', 'other', 'in', 'a', '.', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'a', 'to', 'a', 'a', 'in', 'the', 'other', 'in', 'the', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'a', 'with', 'a', 'man', 'in', 'the', 'other', 'in', 'the', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'on', 'a', 'a', 'in', 'the', 'other', 'in', 'the', 'background', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'on', 'a', 'a', 'in', 'the', 'other', 'in', 'the', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'a', 'to', 'a', 'a', 'in', 'the', 'other', 'in', 'a', '.', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'on', 'a', 'a', 'in', 'the', 'other', 'in', 'a', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'a', 'to', 'a', 'a', 'in', 'the', 'other', 'in', 'a', '.', '.', '<eos>']\n",
      "['<bos>', 'An', 'older', 'man', 'is', 'a', 'a', 'a', 'with', 'a', 'man', 'in', 'the', 'other', 'in', 'a', '.', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for b in beam:\n",
    "    word_list = b[1]\n",
    "    translated_sentence = [vocab_transform[TGT_LANGUAGE].lookup_token(word_id) for word_id in word_list]\n",
    "    print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fk4igjZ-ZIJS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
